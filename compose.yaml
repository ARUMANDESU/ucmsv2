services:
    backend:
        container_name: ucmsv2-backend
        build:
            context: .
            dockerfile: Dockerfile
        ports:
            - 8080:8080
        environment:
            MODE: "${MODE:-dev}"
            PORT: "8080"
            PG_DSN: "postgres://user:password@postgresql:5432/ucms?sslmode=disable"
            S3_ENDPOINT: "http://minio-s3:9000"
            S3_ACCESS_KEY: "ucmsadmin"
            S3_SECRET_KEY: "ucmsadminpass"
            S3_BUCKET: "ucms-avatars"
            S3_REGION: "us-east-1"
            S3_BASE_URL: "http://localhost:9000/ucms-avatars"
            S3_USE_PATH_STYLE: "true"
            ACCESS_TOKEN_SECRET: "${ACCESS_TOKEN_SECRET:-dev_access_secret_key_123456}"
            REFRESH_TOKEN_SECRET: "${REFRESH_TOKEN_SECRET:-dev_refresh_secret_key_123456}"
            INVITATION_TOKEN_SECRET: "${INVITATION_TOKEN_SECRET:-dev_invitation_secret_key_123456}"
            STAFF_INVITATION_BASE_URL: "${STAFF_INVITATION_BASE_URL:-http://localhost:3000/invitations/accept}"
            STAFF_INVITATION_PAGE_URL: "${STAFF_INVITATION_PAGE_URL:-http://localhost:3000/invitations/accept}"
            SERVICE_NAMESPACE: "ucms"
            SERVICE_NAME: "ucms-api"
            SERVICE_VERSION: "${SERVICE_VERSION:-0.1.0}"
            SERVICE_INSTANCE_ID: "backend-1"
            OTEL_EXPORTER_OTLP_ENDPOINT: "http://otel-collector:4317"
            INITIAL_STAFF_EMAIL: "${INITIAL_STAFF_EMAIL:-}"
            INITIAL_STAFF_USERNAME: "${INITIAL_STAFF_USERNAME:-admin}"
            INITIAL_STAFF_PASSWORD: "${INITIAL_STAFF_PASSWORD:-StrongP@ssw0rd}"
            INITIAL_STAFF_BARCODE: "${INITIAL_STAFF_BARCODE:-000000}"
            INITIAL_STAFF_FIRST_NAME: "${INITIAL_STAFF_FIRST_NAME:-Admin}"
            INITIAL_STAFF_LAST_NAME: "${INITIAL_STAFF_LAST_NAME:-User}"
        depends_on:
            postgresql:
                condition: service_healthy
            minio-s3:
                condition: service_healthy
            # otel-collector:
            #     condition: service_healthy
        networks:
            - default
        restart: always
        profiles: [ "app" ]
        healthcheck:
            test: [ "CMD", "wget", "--spider", "-q", "http://localhost:8080/health" ]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s

    postgresql:
        container_name: ucmsv2-postgresql
        image: "postgres:${PG_VERSION:-17-alpine}"
        ports:
            - 8765:5432
        environment:
            POSTGRES_DB: "ucms"
            POSTGRES_USER: "user"
            POSTGRES_PASSWORD: "password"
        volumes:
            - postgresql_data:/var/lib/postgresql/data
        networks:
            - default
        restart: always
        profiles: [ "infra", "storage" ]
        healthcheck:
            test: [ "CMD-SHELL", "pg_isready -U user -d ucms" ]
            interval: 30s
            timeout: 10s
            retries: 5

    minio-s3:
        container_name: ucmsv2-minio
        image: "minio/minio:${MINIO_VERSION:-latest}"
        command: server /data --console-address ":9001"
        ports:
            - 9000:9000
            - 9001:9001
        environment:
            MINIO_ROOT_USER: "ucmsadmin"
            MINIO_ROOT_PASSWORD: "ucmsadminpass"
        volumes:
            - minio_data:/data
        networks:
            - default
        restart: always
        profiles: [ "infra", "storage" ]
        healthcheck:
            test: [ "CMD-SHELL", "curl -f http://localhost:9000/minio/health/live || exit 1" ]
            interval: 30s
            timeout: 10s
            retries: 5

    otel-collector:
        container_name: ucmsv2-otel-collector
        image: "otel/opentelemetry-collector-contrib:${OTEL_VERSION:-latest}"
        command: [ "--config=/etc/otel-collector-config.yaml" ]
        volumes:
            - ./config/otel-collector.yaml:/etc/otel-collector-config.yaml
        ports:
            - 1888:1888 # pprof extension
            - 8888:8888 # Prometheus metrics exposed by the Collector
            - 8889:8889 # Prometheus exporter metrics
            - 13133:13133 # health_check extension
            - 4317:4317 # OTLP gRPC receiver
            - 4318:4318 # OTLP HTTP receiver
            - 55679:55679 # zpages extension
        depends_on:
            prometheus:
                condition: service_healthy
            loki:
                condition: service_healthy
            tempo:
                condition: service_healthy
        networks:
            - default
        restart: always
        profiles: [ "infra" ]
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:13133/health"]
            interval: 30s
            timeout: 10s
            retries: 5

    grafana:
        container_name: ucmsv2-grafana
        image: "grafana/grafana:${GRAFANA_VERSION:-latest}"
        ports:
            - 3000:3000
        environment:
            GF_SECURITY_ADMIN_USER: "admin"
            GF_SECURITY_ADMIN_PASSWORD: "adminpass"
        volumes:
            - grafana_data:/var/lib/grafana
            - ./config/grafana:/etc/grafana/config
            - ./config/grafana/provisioning/datasources/datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml
            - ./config/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
        depends_on:
            prometheus:
                condition: service_healthy
            loki:
                condition: service_healthy
            tempo:
                condition: service_healthy
        networks:
            - default
        restart: always
        profiles: [ "infra" ]
        healthcheck:
            test: [ "CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1" ]
            interval: 30s
            timeout: 10s
            retries: 5

    loki:
        container_name: ucmsv2-loki
        image: "grafana/loki:${LOKI_VERSION:-latest}"
        command: -config.file=/etc/loki/local-config.yaml
        ports:
            - 3100:3100
        volumes:
            - loki_data:/loki
            - ./config/loki.yaml:/etc/loki/local-config.yaml
        networks:
            - default
        restart: always
        profiles: [ "infra" ]
        healthcheck:
            test: [ "CMD", "wget", "--spider", "-q", "http://localhost:3100/ready" ]
            interval: 30s
            timeout: 5s
            retries: 5

    tempo:
        container_name: ucmsv2-tempo
        image: "grafana/tempo:${TEMPO_VERSION:-latest}"
        command: [ "-config.file=/etc/tempo/config.yaml" ]
        ports:
            - 14268 # jaeger ingest
            - 3200 # tempo
            - 4317 # otlp grpc
            - 4318 # otlp http
            - 9411 # zipkin
        volumes:
            - tempo_data:/var/tempo
            - ./config/tempo.yaml:/etc/tempo/config.yaml
        networks:
            - default
        restart: always
        profiles: [ "infra" ]
        healthcheck:
            test: [ "CMD", "wget", "--spider", "-q", "http://localhost:3200/ready" ]
            interval: 30s
            timeout: 5s
            retries: 5

    prometheus:
        container_name: ucmsv2-prometheus
        image: "prom/prometheus:${PROMETHEUS_VERSION:-latest}"
        ports:
            - 9090:9090
        volumes:
            - ./config/prometheus.yaml:/etc/prometheus/prometheus.yml
        networks:
            - default
        restart: always
        profiles: [ "infra" ]
        healthcheck:
            test: [ "CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy" ]
            interval: 30s
            timeout: 10s
            retries: 5

volumes:
    postgresql_data:
    minio_data:
    grafana_data:
    loki_data:
    tempo_data:


networks:
    default:
        name: ucmsv2_default
        driver: bridge
